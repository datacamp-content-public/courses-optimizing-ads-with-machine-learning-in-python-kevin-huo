---
title: Insert title here
key: 2c019d1e7d05eda0ad12c0130582d6ce

---
## Evaluating Logistic Regression

```yaml
type: "TitleSlide"
key: "aa46913cdf"
```

`@lower_third`

name: Kevin Huo
title: DataCamp Instructor


`@script`



---
## Sklearn's metric module

```yaml
type: "FullSlide"
key: "c8e8c52ed8"
disable_transition: true
```

`@part1`
```
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import roc_curve, auc
``` {{1}}
- accuracy_score() looks at accuracy, whereas classification_report() looks at precision and recall {{2}}
- roc_curve() and auc() will be helpful in assessing precision and recall visually {{3}}


`@script`
Sklearn's metric module contains many important functions for evaluating classifiers. We'll be using the following functions. First, the accuracy_score() function will assess model accuracy. Next, the classification_report() function will assess precision and recall, metrics which are important for imbalanced datasets where the number of positive examples may be low. This is usually true for ad click-through rates. Finally, we will be using two other functions, roc_curve() and auc(), in providing a more visual output for how our classifier does. That output is called the ROC curve and will be useful in assessing precision and recall.


---
## The classification_report() and accuracy_score() functions

```yaml
type: "FullSlide"
key: "e7c21f53a7"
```

`@part1`
```
Y_true, Y_pred = Y_test, clf.predict(Y_test)
classification_report(Y_true, Y_pred)
accuracy_score(Y_true, Y_pred)
``` {{1}}
![](https://assets.datacamp.com/production/repositories/5051/datasets/8e3ff55b2b1a0cb93dbc6320ab598940b48988f3/classification_output_sample.png) {{2}}


`@script`
The inputs to the classification_report and accuracy_score() functions are 1. An array called "Y_true" which contains the ground truth labels on our testing examples, and 2. An array called "Y_pred" which contains the output that our logistic regression classifier has for our testing examples. The accuracy_score() function outputs the fraction of labels which match between the model prediction and the actual label. It can be seen in the bottom part of the output. The classification_report() function outputs a text summary of some key metrics for classifiers that looks like the following table when printed. The two metrics we will focus on are precision and recall. Precision is defined as the number of true positives divided by the number of true positives plus false positives. It is a measure of how relevant the results of our model are - of the ones that are labeled positive, how many were actually positive. In this case, it refers to: of the clicks on ads that our model predicted, how many were actual clicks? Recall is defined as the number of true positives divided by the number of true positives plus false negatives. It is a measure of how many truly relevant results our model returned - of the actual universe of ad clicks, how many did our model capture? Ideally we want high values for both, as close to 1 as possible, and we can get a better sense through an ROC Curve.


---
## ROC Curve and AUC

```yaml
type: "FullSlide"
key: "06c48a353f"
disable_transition: false
hide_title: false
```

`@part1`
![](https://assets.datacamp.com/production/repositories/5051/datasets/c47cc159f58003c530598df255cf6b078ae7f392/roc_curve_x_small.png)


`@script`
The ROC (or Receiver Operating Characteristics) curve is a plot that looks like the one shown below, and plots true positive rate on the Y-axis and false positive rate on the X-axis for various threshold levels. The top-left is ideal since it would be best if a classifier had a true positive rate of 1 while a false positive rate of 0. The corresponding area under the curve, or AUC, is a measure of model skill. A higher AUC means a higher precision and a higher recall, based on the definitions from the previous slide. This is because a lower x-value means lower false positives and higher true negatives, and larger y-values means higher true positives and lower false negatives. The dotted blue line represents a baseline for a model with no skill and has a corresponding AUC of 0.5.


---
## Building ROC Curve

```yaml
type: "FullSlide"
key: "a0ae4ca762"
```

`@part1`
```
Y_score = clf.predict_proba(X_test) 
fpr, tpr, thresholds = roc_curve(Y_test, Y_score[:, 1])
roc_auc = auc(fpr, tpr)
``` {{1}}
- Need probabilities, not labels {{2}}
- roc_curve() inputs: test and score arrays {{3}}
- auc() input: false-positive, and true-positive arrays {{4}}


`@script`
To produce the ROC curve, we have to used predicted probabilities from our classifier rather than the predictions themselves when we used classification_report() and accuracy() functions. Remember that the logistic regression model outputs probabilities and we turn them into labels using a threshold for actual predictions. We can use sklearn's predict_proba() function with our classifier, and extract the relevant ROC curve using the roc_curve() function and then the corresponding auc() function to get the area under the ROC curve. Now that you've learned about various metrics and how to evaluate them, let's evaluate our logistic regression classifier!


---
## Let's practice!

```yaml
type: "FinalSlide"
key: "9db2d40be3"
```

`@script`


